{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb-YTKscOBgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic8BEY-kOI5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "    # Positive Reviews\n",
        "\n",
        "    'This is an excellent movie',\n",
        "    'The move was fantastic I like it',\n",
        "    'You should watch it is brilliant',\n",
        "    'Exceptionally good',\n",
        "    'Wonderfully directed and executed I like it',\n",
        "    'Its a fantastic series',\n",
        "    'Never watched such a brillent movie',\n",
        "    'It is a Wonderful movie',\n",
        "\n",
        "    # Negtive Reviews\n",
        "\n",
        "    \"horrible acting\",\n",
        "    'waste of money',\n",
        "    'pathetic picture',\n",
        "    'It was very boring',\n",
        "    'I did not like the movie',\n",
        "    'The movie was horrible',\n",
        "    'I will not recommend',\n",
        "    'The acting is pathetic'\n",
        "]\n",
        "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGLyZESJOSij",
        "colab_type": "text"
      },
      "source": [
        "In the last section, we used one_hot function to convert text to vectors. Another approach is to use Tokenizer function from keras.preprocessing.text library.\n",
        "\n",
        "You simply have to pass your corpus to the Tokenizer's fit_on_text method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV2_5EuKOaFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyUIUElSOyNt",
        "colab_type": "text"
      },
      "source": [
        "#To get the number of unique words in the text\n",
        "You can simply count the length of word_index dictionary of the word_tokenizer object. Remember to add 1 with the vocabulary size. This is to store the dimensions for the words for which no pretrained word embeddings exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55IAuYevO4qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_length = len(word_tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCidpOPoO_ng",
        "colab_type": "text"
      },
      "source": [
        "#Convert sentences to their numeric counterpart\n",
        "Call the texts_to_sequences function and pass it the whole corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plAPqxz1PHUE",
        "colab_type": "code",
        "outputId": "66ff8717-ae6b-4bf6-b8fc-4e56698228ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "embedded_sentences = word_tokenizer.texts_to_sequences(corpus)\n",
        "print(embedded_sentences)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14, 3, 15, 16, 1], [4, 17, 6, 9, 5, 7, 2], [18, 19, 20, 2, 3, 21], [22, 23], [24, 25, 26, 27, 5, 7, 2], [28, 8, 9, 29], [30, 31, 32, 8, 33, 1], [2, 3, 8, 34, 1], [10, 11], [35, 36, 37], [12, 38], [2, 6, 39, 40], [5, 41, 13, 7, 4, 1], [4, 1, 6, 10], [5, 42, 13, 43], [4, 11, 3, 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5zbsn37PZTo",
        "colab_type": "text"
      },
      "source": [
        "#Find the number of words in the longest sentence and then to apply padding to the sentences having shorter lengths than the length of the longest sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5sMflcbPfkC",
        "colab_type": "code",
        "outputId": "fc7f03d9-9645-4bde-b7f2-b0fc3b271957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "word_count = lambda sentence: len(word_tokenize(sentence))\n",
        "longest_sentence = max(corpus, key=word_count)\n",
        "length_long_sentence = len(word_tokenize(longest_sentence))\n",
        "\n",
        "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
        "\n",
        "print(padded_sentences)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[[14  3 15 16  1  0  0]\n",
            " [ 4 17  6  9  5  7  2]\n",
            " [18 19 20  2  3 21  0]\n",
            " [22 23  0  0  0  0  0]\n",
            " [24 25 26 27  5  7  2]\n",
            " [28  8  9 29  0  0  0]\n",
            " [30 31 32  8 33  1  0]\n",
            " [ 2  3  8 34  1  0  0]\n",
            " [10 11  0  0  0  0  0]\n",
            " [35 36 37  0  0  0  0]\n",
            " [12 38  0  0  0  0  0]\n",
            " [ 2  6 39 40  0  0  0]\n",
            " [ 5 41 13  7  4  1  0]\n",
            " [ 4  1  6 10  0  0  0]\n",
            " [ 5 42 13 43  0  0  0]\n",
            " [ 4 11  3 12  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkL1STPj92Kv",
        "colab_type": "text"
      },
      "source": [
        "# Load the GloVe word embeddings and then create our embedding matrix that contains the words in our corpus and their corresponding values from GloVe embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HKYnNU4QtCf",
        "colab_type": "code",
        "outputId": "f33b5c38-a218-4da4-fc0b-c84bbcd1b52e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip ‘glove.6B.zip’ saved [862182613/862182613]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-18 22:24:42--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-18 22:24:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-18 22:24:42--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  2.07MB/s    in 6m 27s  \n",
            "\n",
            "2020-05-18 22:31:09 (2.13 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "--2020-05-18 22:31:09--  http://xn--glove-sv3b.6b.xn--zip-to0a/\n",
            "Resolving xn--glove-sv3b.6b.xn--zip-to0a (xn--glove-sv3b.6b.xn--zip-to0a)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘xn--glove-sv3b.6b.xn--zip-to0a’\n",
            "--2020-05-18 22:31:09--  http://saved/\n",
            "Resolving saved (saved)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘saved’\n",
            "http://[862182613/862182613]: Invalid IPv6 numeric address.\n",
            "FINISHED --2020-05-18 22:31:09--\n",
            "Total wall clock time: 6m 27s\n",
            "Downloaded: 1 files, 822M in 6m 27s (2.13 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI7KfKbGy1rR",
        "colab_type": "code",
        "outputId": "68d40d51-7ade-4c0c-99d5-d6cc207a8fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3qJRX3A1JpH",
        "colab_type": "code",
        "outputId": "6a09758d-57b2-48cb-fb2a-138aa6e641c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip    sample_data\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   glove.6B.zip.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txNmbDQR1Z6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxiXiIGw1RIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hxGgkVL-CUP",
        "colab_type": "text"
      },
      "source": [
        "# Create a dictionary that will contain words as keys and the corresponding 100 dimensional vectors as values, in the form of an array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQLNd3E1sbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "\n",
        "glove_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3bl9-YZ_t5J",
        "colab_type": "text"
      },
      "source": [
        "We want the word embeddings for only those words that are present in our corpus. We will create a two dimensional numpy array of 44 (size of vocabulary) rows and 100 columns. The array will initially contain zeros. The array will be named as embedding_matrix\n",
        "\n",
        "Next, we will iterate through each word in our corpus by traversing the word_tokenizer.word_index dictionary that contains our words and their corresponding index.\n",
        "\n",
        "Each word will be passed as key to the embedding_dictionary to retrieve the corresponding 100 dimensional vector for the word. The 100 dimensional vector will then be stored at the corresponding index of the word in the embedding_matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNar79LB3pRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = zeros((vocab_length, 100))\n",
        "for word, index in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfpRQQS-89H2",
        "colab_type": "text"
      },
      "source": [
        "# Create and compile our sequential model  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yvy3v4e9GpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi1aSj5Z9JuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "9400e9af-32b2-44e1-ec88-4324188a3c94"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 7, 100)            4400      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 700)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 701       \n",
            "=================================================================\n",
            "Total params: 5,101\n",
            "Trainable params: 701\n",
            "Non-trainable params: 4,400\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWpFGn5W9hY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fb8f5f7-33fe-4999-c07b-d5b42e658bb1"
      },
      "source": [
        "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.6939 - acc: 0.6250\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 291us/step - loss: 0.6755 - acc: 0.6875\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 261us/step - loss: 0.6549 - acc: 0.6875\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 335us/step - loss: 0.6339 - acc: 0.6875\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 297us/step - loss: 0.6129 - acc: 0.6875\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 150us/step - loss: 0.5923 - acc: 0.7500\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 115us/step - loss: 0.5723 - acc: 0.7500\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 315us/step - loss: 0.5528 - acc: 0.7500\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 107us/step - loss: 0.5338 - acc: 0.8125\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 288us/step - loss: 0.5156 - acc: 0.8125\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 215us/step - loss: 0.4980 - acc: 0.8125\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 214us/step - loss: 0.4812 - acc: 0.8750\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 126us/step - loss: 0.4651 - acc: 0.9375\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 136us/step - loss: 0.4495 - acc: 0.9375\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 134us/step - loss: 0.4346 - acc: 0.9375\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 109us/step - loss: 0.4202 - acc: 0.9375\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 120us/step - loss: 0.4064 - acc: 0.9375\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 150us/step - loss: 0.3933 - acc: 0.9375\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 112us/step - loss: 0.3806 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 126us/step - loss: 0.3685 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 123us/step - loss: 0.3568 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 160us/step - loss: 0.3455 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 141us/step - loss: 0.3347 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 175us/step - loss: 0.3244 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 160us/step - loss: 0.3144 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 115us/step - loss: 0.3050 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 166us/step - loss: 0.2958 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 169us/step - loss: 0.2870 - acc: 1.0000\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 211us/step - loss: 0.2785 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 228us/step - loss: 0.2704 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 204us/step - loss: 0.2626 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 144us/step - loss: 0.2551 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 221us/step - loss: 0.2479 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 145us/step - loss: 0.2410 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 206us/step - loss: 0.2344 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 245us/step - loss: 0.2280 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 198us/step - loss: 0.2219 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 130us/step - loss: 0.2160 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 140us/step - loss: 0.2103 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 133us/step - loss: 0.2049 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 127us/step - loss: 0.1996 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 99us/step - loss: 0.1946 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 174us/step - loss: 0.1897 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 497us/step - loss: 0.1851 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 167us/step - loss: 0.1806 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 128us/step - loss: 0.1762 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 173us/step - loss: 0.1721 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 149us/step - loss: 0.1681 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 173us/step - loss: 0.1642 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 139us/step - loss: 0.1605 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 186us/step - loss: 0.1569 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 197us/step - loss: 0.1534 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 178us/step - loss: 0.1500 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 174us/step - loss: 0.1468 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 176us/step - loss: 0.1437 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 132us/step - loss: 0.1407 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 136us/step - loss: 0.1378 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 140us/step - loss: 0.1349 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 216us/step - loss: 0.1322 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 347us/step - loss: 0.1296 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 133us/step - loss: 0.1270 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 128us/step - loss: 0.1245 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 132us/step - loss: 0.1221 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 169us/step - loss: 0.1198 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 123us/step - loss: 0.1175 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 131us/step - loss: 0.1154 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 135us/step - loss: 0.1132 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 134us/step - loss: 0.1112 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 229us/step - loss: 0.1092 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 161us/step - loss: 0.1073 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 161us/step - loss: 0.1054 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 148us/step - loss: 0.1035 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 152us/step - loss: 0.1018 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 128us/step - loss: 0.1001 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 142us/step - loss: 0.0984 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 273us/step - loss: 0.0967 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 519us/step - loss: 0.0952 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 156us/step - loss: 0.0936 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 305us/step - loss: 0.0921 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 176us/step - loss: 0.0907 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 169us/step - loss: 0.0892 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 149us/step - loss: 0.0878 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 120us/step - loss: 0.0865 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 208us/step - loss: 0.0852 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 208us/step - loss: 0.0839 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 205us/step - loss: 0.0826 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 216us/step - loss: 0.0814 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 156us/step - loss: 0.0802 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 140us/step - loss: 0.0791 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 146us/step - loss: 0.0779 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 135us/step - loss: 0.0768 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 146us/step - loss: 0.0757 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 195us/step - loss: 0.0747 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 201us/step - loss: 0.0736 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 194us/step - loss: 0.0726 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 155us/step - loss: 0.0716 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 231us/step - loss: 0.0707 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 232us/step - loss: 0.0697 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 145us/step - loss: 0.0688 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 154us/step - loss: 0.0679 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f7d919be6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV3l6-gM9lJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d237860-fdcc-4cf2-f06a-affc0c85c394"
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 100.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikdCqnyi-7sb",
        "colab_type": "text"
      },
      "source": [
        "Jupyter Notebook implementation of the article [Python for NLP: Word Embeddings for Deep Learning in Keras](https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/) by [Usman Malik](https://stackabuse.com/author/usman/)"
      ]
    }
  ]
}